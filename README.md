
# Desafio para engenheiro de dados iniciante
<h1 align="center">
<img src="img/fiec.png">
<p>Projeto: Engenheiro de dados SFIEC‚Äì Jammesson Cabral
</p>
</h1>


# üìñ Sobre  Etapas do projeto
<p align="center">
- <a href="#Data-Lake">Data-Lake</a>
- <a href="#extra√ß√£o">Extra√ß√£o</a>
- <a href="#transforma√ß√£o">Transforma√ß√£o</a>
- <a href="#carregar">Carregar</a>
- <a href="#analises">Analises</a>
</p>


# Data-Lake 
<p>
Sobre a estrutura do Data Lake definir trabalhar com 3 zonas nomeadas como (Bronze, Prata e Ouro).
Estas zonas seviram para organizar os arquivos dentro delas, bem como definir permiss√µes de acesso para equipe.
Segue estrutura de cada zona:
</p>

-ü•â **Zona Bronze**: Local onde vamos guardar dados brutos completos de como eles s√£o feito a extra√ß√£o.

-ü•à **Zona Prata**: Local onde vamos fazer as transforma√ß√µes necessarias para o projeto bem como limpeza de arquivos que n√£o seram utilizados.

-ü•á **Zona Ouro**: Local onde vamos disponibilizar arquivos j√° tratados para equipe de Ciencia de dado / Analista de dados fazerem seus trabalhos.

<h1 align="center">
<img src="img/sfiec_jam.png">
</h1>

# Extra√ß√£o
- <p>Sobre a extra√ß√£o verificamos que o link os dados do Anu√°rio Estat√≠sticos da ANTAQ (Ag√™ncia Nacional de Transportes Aqu√°ticos) existia um padr√£o a ser seguido, ent√£o entendemos que padr√£o foi, onde o link para download √© o mesmo s√≥ muda uma string que seria o ano referente ao ano em quest√£o. Com esse padr√£o em m√£os, definimos uma lista com os anos que precisavam ser extraidos, e utilizamos o wget para fazer o download dos arquivos .zip para a zona bronze mudando apenas a variavel chave do link o [ANO] </p>

<p>Nesta Etapa tamb√©m j√° copiamos todos os arquivos .zip para pasta prata e fizendo o unzip, deletamos os arquivos que n√£o seriam utilizados pela equipe de ci√™ncia de dados bem como os arquivos zip, deixando assim s√≥ os arquivos atracacao.txt e carga.txt de cada ano.  </p>


# Transforma√ß√£o

- Nesta etapa vamos come√ßar a fazer as limpezas dos dados.
Carregamos todos os arquivos do ano de 2019 2020 e 2021 conforme a equipe solicitou em um pandas dataframe.
- Verificamos que algumas colunas no farmato datatime n√£o tinham sida reconhecidas como padr√£o no dataframe.
- Alteramos as colunas de datatime no dataframe.
- Verificamos algumas linhas nulas, mas n√£o excluimos nenhuma pois n√£o sabemos quais variaveis s√£o importantes para a analise, n√£o foi definido se a gente coloca algum dado padr√£o para esses campos nulos ent√£o optamos por deixar.

# Carregar

- Nesta etapa vamos carregar os dados de um dataframe para o SQL SERVER.
- Ap√≥s tentar importar os dados para o SQL Server verificamos que os campos nulos estavam dando erro, ent√£o mudando os campos do dataframe NaN, para None onde conseguimos fazer com que o banco enteda que esse campo √© nulo.

# Analises

- Verificamos na hora de inserir os dados da tabela Atracacao_fato para o SQL Server, demorava em torno de 20 minutos, esta tabela tem o total de 232.944 linhas e 18 colunas, enquanto a tabela carga_fato tem 6.479.692 linhas e 16 colunas, ou seja iriamos passar em torno de 9 ou mais horas.
- Sabemos que tudo depende da for√ßa do hardware tanto o que est√° do lado do servidor SQL como o que est√° processando os dados e enviando para o SQL ent√£o talvez uma maquina mais robusta diminuiria este tempo.
- Tamb√©m verificamos que pelo fato dessa quantidade de linhas, j√° temos uma quantidade significante para serem inseridos e analisados, verificando que o processamento paralelo poderia ser uma boa alternativa como spark utilizando os arquivos parquet ou avro.



